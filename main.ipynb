{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "from string import punctuation\n",
    "\n",
    "import nltk\n",
    "from nltk.stem.snowball import SnowballStemmer as stemmer_fn\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation\n",
    "from tensorflow.keras import utils\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data & configure NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/kuba/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "LANG = 'english'\n",
    "\n",
    "stemmer = stemmer_fn(LANG)\n",
    "nltk.download('punkt')\n",
    "\n",
    "import json\n",
    "with open('intents.json') as json_data:\n",
    "    intents = json.load(json_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract corpus & intent from json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33 documents\n",
      "8 classes ['More education', 'More experience', 'education', 'experience', 'goodbye', 'greeting', 'noanswer', 'thanks']\n",
      "48 unique stemmed words [\"'s\", 'about', 'anyon', 'are', 'befor', 'bye', 'cheer', 'day', 'degre', 'did', 'do', 'educ', 'elabor', 'exact', 'experi', 'good', 'goodby', 'have', 'hello', 'help', 'hey', 'hi', 'how', 'is', 'it', 'later', 'me', 'more', 'now', 'obtain', 'on', 'project', 'right', 'see', 'so', 'someth', 'somewher', 'studi', 'tell', 'thank', 'that', 'the', 'there', 'what', 'where', 'work', 'you', 'your']\n"
     ]
    }
   ],
   "source": [
    "words = []\n",
    "documents = []\n",
    "classes = sorted(list(set([intent['tag'] for intent in intents['intents']])))\n",
    "stop_words = set(list(punctuation))\n",
    "\n",
    "def pre_process_words(wrds, stop):\n",
    "    return [stemmer.stem(w.lower()) for w in wrds if w not in stop]\n",
    "\n",
    "def pre_process_sentence(sentence, stop):\n",
    "    wrds = nltk.word_tokenize(sentence)\n",
    "    return [stemmer.stem(w.lower()) for w in wrds if w not in stop]\n",
    "\n",
    "def bow_fn(sentence, words):\n",
    "    wrds = pre_process_sentence(sentence, stop_words)\n",
    "    bag = np.zeros((num_words))\n",
    "\n",
    "    for i,w in enumerate(words):\n",
    "        if w in wrds:\n",
    "            bag[i] = 1\n",
    "            \n",
    "    return bag\n",
    "            \n",
    "# Go over the intents and their respective patterns\n",
    "for intent in intents['intents']:\n",
    "    for pattern in intent['patterns']:\n",
    "\n",
    "        # tokenize patterns & skip stop words\n",
    "        wrds = nltk.word_tokenize(pattern)\n",
    "        words.extend(wrds)\n",
    "\n",
    "        # create pairs (tokenized sentence, intent)\n",
    "        documents.append((wrds, intent['tag']))\n",
    "\n",
    "'''\n",
    "dictionary of words\n",
    "- stemmed\n",
    "- lowercase\n",
    "- not in stop_words list\n",
    "'''\n",
    "words = pre_process_words(words, stop_words)\n",
    "words = sorted(list(set(words)))\n",
    "\n",
    "print(len(documents), \"documents\")\n",
    "print(len(classes), \"classes\", classes)\n",
    "print(len(words), \"unique stemmed words\", words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_documents = len(documents)\n",
    "num_classes = len(classes)\n",
    "num_words = len(words)\n",
    "num_classes = len(classes)\n",
    "\n",
    "X = np.zeros((num_documents, num_words))\n",
    "y = np.zeros((num_documents, num_classes))\n",
    "\n",
    "# training set, bag of words for each sentence\n",
    "for j,doc in enumerate(documents):\n",
    "    wrds, intent = doc\n",
    "    wrds = pre_process_words(wrds, stop_words)\n",
    "    \n",
    "    for i,w in enumerate(words):\n",
    "        if w in wrds:\n",
    "            X[j,i] = 1\n",
    "\n",
    "    y[j,classes.index(intent)] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build & train simple model \n",
    "\n",
    "Here I actually don't mind the model overfitting the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_16 (Dense)             (None, 10)                490       \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 10)                110       \n",
      "_________________________________________________________________\n",
      "dense_18 (Dense)             (None, 10)                110       \n",
      "_________________________________________________________________\n",
      "dense_19 (Dense)             (None, 8)                 88        \n",
      "=================================================================\n",
      "Total params: 798\n",
      "Trainable params: 798\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 33 samples\n",
      "Epoch 1/50\n",
      "33/33 [==============================] - 1s 18ms/sample - loss: 2.0387 - acc: 0.2727\n",
      "Epoch 2/50\n",
      "33/33 [==============================] - 0s 665us/sample - loss: 2.0096 - acc: 0.3333\n",
      "Epoch 3/50\n",
      "33/33 [==============================] - 0s 766us/sample - loss: 1.9860 - acc: 0.3333\n",
      "Epoch 4/50\n",
      "33/33 [==============================] - 0s 700us/sample - loss: 1.9635 - acc: 0.3333\n",
      "Epoch 5/50\n",
      "33/33 [==============================] - 0s 735us/sample - loss: 1.9437 - acc: 0.3333\n",
      "Epoch 6/50\n",
      "33/33 [==============================] - 0s 816us/sample - loss: 1.9181 - acc: 0.3333\n",
      "Epoch 7/50\n",
      "33/33 [==============================] - 0s 832us/sample - loss: 1.9002 - acc: 0.3939\n",
      "Epoch 8/50\n",
      "33/33 [==============================] - 0s 723us/sample - loss: 1.8756 - acc: 0.3939\n",
      "Epoch 9/50\n",
      "33/33 [==============================] - 0s 983us/sample - loss: 1.8555 - acc: 0.3333\n",
      "Epoch 10/50\n",
      "33/33 [==============================] - 0s 1ms/sample - loss: 1.8338 - acc: 0.3333\n",
      "Epoch 11/50\n",
      "33/33 [==============================] - 0s 603us/sample - loss: 1.8147 - acc: 0.3333\n",
      "Epoch 12/50\n",
      "33/33 [==============================] - 0s 488us/sample - loss: 1.7930 - acc: 0.3939\n",
      "Epoch 13/50\n",
      "33/33 [==============================] - 0s 890us/sample - loss: 1.7698 - acc: 0.3939\n",
      "Epoch 14/50\n",
      "33/33 [==============================] - 0s 990us/sample - loss: 1.7492 - acc: 0.3939\n",
      "Epoch 15/50\n",
      "33/33 [==============================] - 0s 1ms/sample - loss: 1.7269 - acc: 0.3939\n",
      "Epoch 16/50\n",
      "33/33 [==============================] - 0s 692us/sample - loss: 1.7053 - acc: 0.3939\n",
      "Epoch 17/50\n",
      "33/33 [==============================] - 0s 511us/sample - loss: 1.6821 - acc: 0.4242\n",
      "Epoch 18/50\n",
      "33/33 [==============================] - 0s 697us/sample - loss: 1.6632 - acc: 0.4242\n",
      "Epoch 19/50\n",
      "33/33 [==============================] - 0s 928us/sample - loss: 1.6391 - acc: 0.4242\n",
      "Epoch 20/50\n",
      "33/33 [==============================] - 0s 976us/sample - loss: 1.6161 - acc: 0.4848\n",
      "Epoch 21/50\n",
      "33/33 [==============================] - 0s 594us/sample - loss: 1.5936 - acc: 0.4848\n",
      "Epoch 22/50\n",
      "33/33 [==============================] - 0s 886us/sample - loss: 1.5706 - acc: 0.4848\n",
      "Epoch 23/50\n",
      "33/33 [==============================] - 0s 824us/sample - loss: 1.5482 - acc: 0.5152\n",
      "Epoch 24/50\n",
      "33/33 [==============================] - 0s 755us/sample - loss: 1.5284 - acc: 0.5455\n",
      "Epoch 25/50\n",
      "33/33 [==============================] - 0s 601us/sample - loss: 1.5033 - acc: 0.5455\n",
      "Epoch 26/50\n",
      "33/33 [==============================] - 0s 721us/sample - loss: 1.4810 - acc: 0.6061\n",
      "Epoch 27/50\n",
      "33/33 [==============================] - 0s 840us/sample - loss: 1.4546 - acc: 0.5455\n",
      "Epoch 28/50\n",
      "33/33 [==============================] - 0s 585us/sample - loss: 1.4300 - acc: 0.5455\n",
      "Epoch 29/50\n",
      "33/33 [==============================] - 0s 1ms/sample - loss: 1.4057 - acc: 0.6061\n",
      "Epoch 30/50\n",
      "33/33 [==============================] - 0s 725us/sample - loss: 1.3810 - acc: 0.6364\n",
      "Epoch 31/50\n",
      "33/33 [==============================] - 0s 879us/sample - loss: 1.3552 - acc: 0.6364\n",
      "Epoch 32/50\n",
      "33/33 [==============================] - 0s 869us/sample - loss: 1.3344 - acc: 0.6364\n",
      "Epoch 33/50\n",
      "33/33 [==============================] - 0s 782us/sample - loss: 1.3077 - acc: 0.6364\n",
      "Epoch 34/50\n",
      "33/33 [==============================] - 0s 789us/sample - loss: 1.2830 - acc: 0.6061\n",
      "Epoch 35/50\n",
      "33/33 [==============================] - 0s 819us/sample - loss: 1.2591 - acc: 0.6061\n",
      "Epoch 36/50\n",
      "33/33 [==============================] - 0s 741us/sample - loss: 1.2331 - acc: 0.6364\n",
      "Epoch 37/50\n",
      "33/33 [==============================] - 0s 954us/sample - loss: 1.2088 - acc: 0.6667\n",
      "Epoch 38/50\n",
      "33/33 [==============================] - 0s 786us/sample - loss: 1.1829 - acc: 0.6667\n",
      "Epoch 39/50\n",
      "33/33 [==============================] - 0s 818us/sample - loss: 1.1575 - acc: 0.6667\n",
      "Epoch 40/50\n",
      "33/33 [==============================] - 0s 551us/sample - loss: 1.1370 - acc: 0.6970\n",
      "Epoch 41/50\n",
      "33/33 [==============================] - 0s 925us/sample - loss: 1.1145 - acc: 0.7273\n",
      "Epoch 42/50\n",
      "33/33 [==============================] - 0s 877us/sample - loss: 1.0915 - acc: 0.7273\n",
      "Epoch 43/50\n",
      "33/33 [==============================] - 0s 735us/sample - loss: 1.0676 - acc: 0.7576\n",
      "Epoch 44/50\n",
      "33/33 [==============================] - 0s 671us/sample - loss: 1.0438 - acc: 0.7879\n",
      "Epoch 45/50\n",
      "33/33 [==============================] - 0s 823us/sample - loss: 1.0207 - acc: 0.8182\n",
      "Epoch 46/50\n",
      "33/33 [==============================] - 0s 758us/sample - loss: 0.9990 - acc: 0.8182\n",
      "Epoch 47/50\n",
      "33/33 [==============================] - 0s 690us/sample - loss: 0.9787 - acc: 0.8182\n",
      "Epoch 48/50\n",
      "33/33 [==============================] - 0s 552us/sample - loss: 0.9626 - acc: 0.7879\n",
      "Epoch 49/50\n",
      "33/33 [==============================] - 0s 590us/sample - loss: 0.9426 - acc: 0.7879\n",
      "Epoch 50/50\n",
      "33/33 [==============================] - 0s 640us/sample - loss: 0.9196 - acc: 0.7879\n"
     ]
    }
   ],
   "source": [
    "idx = np.arange(num_documents)\n",
    "random.shuffle(idx)\n",
    "\n",
    "X = X[idx]\n",
    "y = y[idx]\n",
    "\n",
    "num_neurons = 10\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(num_neurons, input_shape=(X.shape[1],)))\n",
    "model.add(Dense(num_neurons))\n",
    "model.add(Dense(num_neurons))\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "model.summary()\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['acc'])\n",
    "history = model.fit(np.array(X), np.array(y), epochs=50, batch_size=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "context = {}\n",
    "\n",
    "def inference(sentence, threshold, show_details=False):\n",
    "    p = bow_fn(sentence, words)\n",
    "    p = np.expand_dims(p,axis=0)\n",
    "\n",
    "    results = model.predict(p)[0]\n",
    "    y_pred = np.argmax(results)\n",
    "    \n",
    "    if show_details:\n",
    "        print(results, y_pred)\n",
    "    \n",
    "    if results[y_pred] > threshold:\n",
    "        return y_pred\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "goodbye\n"
     ]
    }
   ],
   "source": [
    "text = 'Talk to you later jakub'\n",
    "\n",
    "threshold = 0.3\n",
    "t = inference(text, threshold)\n",
    "print(classes[t])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def response(sentence, userID='user_ID', show_details=False):\n",
    "    results = inference(sentence, threshold, show_details)\n",
    "\n",
    "    if results is not None:\n",
    "        intent_pred = classes[results]\n",
    "\n",
    "        for intent in intents['intents']:\n",
    "            if intent['tag'] == intent_pred:\n",
    "                if 'context_set' in intent:\n",
    "                    context[userID] = intent['context_set']\n",
    "\n",
    "                    if show_details: \n",
    "                        print ('context:', intent['context_set'])\n",
    "\n",
    "                # check if this intent is contextual and applies to this user's conversation\n",
    "                if not 'context_filter' in intent or \\\n",
    "                    (userID in context and 'context_filter' in intent and intent['context_filter'] == context[userID]):\n",
    "                    if show_details: \n",
    "                        print('tag:', intent['tag'])\n",
    "\n",
    "                    return print(random.choice(intent['responses']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['More education', 'More experience', 'education', 'experience', 'goodbye', 'greeting', 'noanswer', 'thanks']\n",
      "\n",
      "\n",
      " {}\n",
      "[1.5713117e-03 1.0105671e-06 9.8102254e-01 1.7396430e-02 8.7539702e-06\n",
      " 1.1174781e-14 2.5669622e-16 2.9150478e-12] 2\n",
      "context: more_education\n",
      "tag: education\n",
      "I recently graduated from master degree.\n",
      "\n",
      "\n",
      " {'123': 'more_education'}\n",
      "[6.0428804e-01 3.9481246e-01 4.5078650e-06 3.7309271e-04 5.2438446e-07\n",
      " 3.2260607e-07 6.3809952e-05 4.5733442e-04] 0\n",
      "tag: More education\n",
      "Bachelor in Data Science & master in Mathematical Modelling and Computation\n",
      "[2.7577818e-07 4.2676228e-07 1.8495129e-12 2.7759708e-16 8.0395889e-16\n",
      " 5.5385695e-05 5.4139377e-09 9.9994397e-01] 7\n",
      "context: \n",
      "tag: thanks\n",
      "Any time!\n"
     ]
    }
   ],
   "source": [
    "print(classes)\n",
    "\n",
    "context = {}\n",
    "print('\\n\\n',context)\n",
    "response(\"What's your education\", userID='123', show_details=True)\n",
    "print('\\n\\n',context)\n",
    "response(\"tell me more about it\", userID='123', show_details=True)\n",
    "response('alright thanks', userID='123', show_details=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I hold BSc & MSc\n",
      "My pleasure\n",
      "You're welcome!\n",
      "Have a nice day\n",
      "\n",
      "\n",
      "\n",
      "I worked as ML engineer.\n",
      "I worked on medical image segmentation but also news article classification.\n",
      "Sample projects are listed on my portfolio website: https://jakubczerny.wixsite.com/portfolio\n",
      "You're welcome!\n",
      "You're welcome!\n",
      "Have a nice day\n"
     ]
    }
   ],
   "source": [
    "context = {}\n",
    "response(\"What's your education\", userID='12345')\n",
    "response(\"tell me more\", userID='12345')\n",
    "response(\"tell me more\", userID='12345')\n",
    "response('alright thanks', userID='12345')\n",
    "response(\"cheers\")\n",
    "response(\"bye\")\n",
    "\n",
    "print('\\n\\n')\n",
    "response(\"Where do you work\", userID='15')\n",
    "response(\"tell me more\", userID='15')\n",
    "response(\"tell me more\", userID='15')\n",
    "response('alright thanks', userID='15')\n",
    "response(\"cheers\")\n",
    "response(\"bye\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
